{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoENqqNjqFNJ"
      },
      "source": [
        "# Метрика через callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCKsOGN5ANvJ"
      },
      "source": [
        "Давайте вспомним, что как мы создаем и обучаем нейронные\n",
        " сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm5DVohaAbGn"
      },
      "source": [
        "Небольшой пример- создаем и компилируем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd90CGP5AeD0"
      },
      "source": [
        "# Простая Dense сеть\n",
        "input1 = Input((xTrainScaled.shape[1],))\n",
        "input2 = Input((xTrainC01.shape[1],))\n",
        "\n",
        "x1 = Dense(10, activation=\"relu\")(input1)\n",
        "x2 = Dense(250, activation=\"relu\")(input2)\n",
        "\n",
        "x = concatenate([x1, x2])\n",
        "\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(10, activation='relu')(x)\n",
        "x = Dense(1, activation='linear')(x)\n",
        "\n",
        "model = Model((input1, input2), x)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=1e-3), loss='mse',metrics = 'mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdp72L7rAe3Y"
      },
      "source": [
        "В процессе обучения оптимизатор стремится минимизировать потери - loss. Алгоритм подсчета этих потерь мы можем выбрать из списка готовых - MAE, MSE, BinaryCrossentropy или придумать свой - уникальный. Алгоритм задается при компиляции модели. За него отвечает параметр loss метода compile.\n",
        "\n",
        "Процесс обучения можно характеризовать не только поведением функции потерь, но и другими параметрами. Они называются метрики.\n",
        "\n",
        "Метрики также можно выбирать из списка готовых - MAE, MSE, BinaryCrossentropy, accuracy или писать самому.\n",
        "Бывает полезным выбирать функцию потерь одного вида, а метрики другого. Например, MAE и MSE. В примере выше так и сделано."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaG1uujGq5bn"
      },
      "source": [
        "[Создание собственной функции ошибки\\метрики](https://colab.research.google.com/drive/1FelcMWA_g8IbICzs4j3w3vf1JfuxymNR?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgGy-W4YErFG"
      },
      "source": [
        "Давайте организуем обработку параметров в процессе обучения для нашей задачи. Просто пересчитаем MAE в удобный нам вид. Для этого напишем callback. В процессе  обучении сети возникают события\n",
        "- начало эпохи\n",
        "- конец эпохи\n",
        "- начало батча\n",
        "- конец батча\n",
        "и т.д.\n",
        "\n",
        "Полный список в описании API KERAS, на эти события можно навешивать их обработчики. Это просто функции, которые будут выполняться при наступлении какого-либо события (конец эпохи, например)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoDV7vlOGl0h"
      },
      "source": [
        "Определим наш callback следующим образом - он будет срабатывать по окончании эпохи\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_45rxJ6zqc4B"
      },
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "\n",
        "  print(epoch, logs)\n",
        "\n",
        "  pred = model.predict([xTrainScaled[valMask], xTrainC01[valMask]])\n",
        "\n",
        "  predUnscaled = yScaler.inverse_transform(pred).flatten()\n",
        "\n",
        "  yTrainUnscaled = yScaler.inverse_transform(yTrainScaled[valMask]).flatten()\n",
        "\n",
        "  delta = predUnscaled - yTrainUnscaled\n",
        "\n",
        "  absDelta = abs(delta)\n",
        "\n",
        "  print(\"Эпоха\", epoch, \"модуль ошибки\", round(sum(absDelta) / (1e+6 * len(absDelta)),3))\n",
        "\n",
        "### Коллбэки\n",
        "pltMae = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K1sirEwErHa"
      },
      "source": [
        "включим наш callback в процесс обучения сетки\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYdXMM4Rqlug"
      },
      "source": [
        "history = model.fit([xTrainScaled[~valMask],\n",
        "\n",
        "                    xTrainC01[~valMask]], yTrainScaled[~valMask],\n",
        "\n",
        "                    epochs=200,\n",
        "\n",
        "                    validation_data=([xTrainScaled[valMask], xTrainC01[valMask]],\n",
        "\n",
        "                    yTrainScaled[valMask]),\n",
        "\n",
        "                    vrbose=0,\n",
        "\n",
        "                    callbacks=[pltMae])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CWPuM5Iano"
      },
      "source": [
        "Наш callback\n",
        "\n",
        "**def on_epoch_end(epoch, logs)**\n",
        "\n",
        "принимает параметры **epoch, logs**\n",
        "\n",
        "**epoch** - номер эпохи: 0,1,2,3...\n",
        "\n",
        "**logs** - словарь с параметрами обучения данной эпохи. В нем потери и метрики.\n",
        "\n",
        "Например:\n",
        "\n",
        "{'loss': 0.5059476494789124, 'mae': 0.11555033922195435, 'val_loss': 0.7494572997093201, 'val_mae': 0.10996615886688232}\n",
        "\n",
        "Заметим, что истории всего обучения он не содержит\n",
        "\n",
        "Этот словарь можно использовать для расчетов и построения графиков после каждой эпохи (в нашем случае)."
      ]
    }
  ]
}